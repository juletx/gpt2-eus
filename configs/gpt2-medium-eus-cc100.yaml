# model arguments
model_type: gpt2
config_overrides:
  vocab_size: 50304
  n_embd: 1024
  n_head: 16
  n_layer: 24
  resid_pdrop: 0.3
  embd_pdrop: 0.3
  attn_pdrop: 0.3
tokenizer_name: HiTZ/gpt2-eus-cc100

# dataset arguments
dataset_name: cc100
dataset_config_name: lang='eu'
validation_split_percentage: 0.1

# checkpoint settings
output_dir: out/gpt2-medium-eus-cc100
overwrite_output_dir: true
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
save_steps: 500
save_total_limit: 4

# evaluation
do_train: true
do_eval: true
evaluation_strategy: steps
logging_steps: 10
eval_steps: 500

# batch size: 15 batch size * 1024 block size * 8 gradaccum * 4 GPUs = 491,520
per_device_train_batch_size: 15
per_device_eval_batch_size: 15
gradient_accumulation_steps: 8
auto_find_batch_size: true

# optimizer settings
optim: adamw_torch
learning_rate: 0.0003
weight_decay: 0.1
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
max_steps: 600000
lr_scheduler_type: cosine
warmup_steps: 2000

# reporting
report_to: wandb
run_name: gpt2-medium-eus-cc100

# hub settings
push_to_hub: true
resume_from_checkpoint: false
hub_model_id: HiTZ/gpt2-medium-eus-cc100
hub_private_repo: true

# performance
bf16: true
torch_compile: true
ddp_find_unused_parameters: false
